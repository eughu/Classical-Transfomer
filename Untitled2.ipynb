{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0634cfcc-c652-47d6-91f6-e0db6379f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Training Loss: 11.663 Validation Loss: 11.716\n",
      "Step: 20 Training Loss: 10.356 Validation Loss: 10.419\n",
      "Step: 40 Training Loss: 8.815 Validation Loss: 8.968\n",
      "Step: 60 Training Loss: 7.44 Validation Loss: 7.818\n",
      "Step: 80 Training Loss: 6.88 Validation Loss: 7.135\n",
      "Step: 100 Training Loss: 6.586 Validation Loss: 7.249\n",
      "Step: 120 Training Loss: 6.472 Validation Loss: 7.065\n",
      "Step: 140 Training Loss: 6.44 Validation Loss: 6.948\n",
      "Step: 160 Training Loss: 6.265 Validation Loss: 6.966\n",
      "Step: 180 Training Loss: 6.173 Validation Loss: 6.738\n",
      "Step: 200 Training Loss: 5.971 Validation Loss: 6.585\n",
      "Step: 220 Training Loss: 5.98 Validation Loss: 6.622\n",
      "Step: 240 Training Loss: 5.97 Validation Loss: 6.411\n",
      "Step: 260 Training Loss: 5.717 Validation Loss: 6.494\n",
      "Step: 280 Training Loss: 5.785 Validation Loss: 6.471\n",
      "Step: 300 Training Loss: 5.725 Validation Loss: 6.363\n",
      "Step: 320 Training Loss: 5.727 Validation Loss: 6.36\n",
      "Step: 340 Training Loss: 5.426 Validation Loss: 6.46\n",
      "Step: 360 Training Loss: 5.521 Validation Loss: 6.213\n",
      "Step: 380 Training Loss: 5.526 Validation Loss: 6.172\n",
      "Step: 400 Training Loss: 5.302 Validation Loss: 5.977\n",
      "Step: 420 Training Loss: 5.422 Validation Loss: 5.994\n",
      "Step: 440 Training Loss: 5.239 Validation Loss: 6.341\n",
      "Step: 460 Training Loss: 5.389 Validation Loss: 6.095\n",
      "Step: 480 Training Loss: 5.169 Validation Loss: 6.074\n",
      "Step: 500 Training Loss: 5.06 Validation Loss: 6.077\n",
      "Step: 520 Training Loss: 5.215 Validation Loss: 5.95\n",
      "Step: 540 Training Loss: 5.085 Validation Loss: 5.894\n",
      "Step: 560 Training Loss: 4.682 Validation Loss: 5.954\n",
      "Step: 580 Training Loss: 5.106 Validation Loss: 6.097\n",
      "Step: 600 Training Loss: 4.942 Validation Loss: 5.884\n",
      "Step: 620 Training Loss: 4.808 Validation Loss: 5.596\n",
      "Step: 640 Training Loss: 4.927 Validation Loss: 5.874\n",
      "Step: 660 Training Loss: 4.655 Validation Loss: 5.9\n",
      "Step: 680 Training Loss: 4.789 Validation Loss: 5.837\n",
      "Step: 700 Training Loss: 4.679 Validation Loss: 5.758\n",
      "Step: 720 Training Loss: 4.82 Validation Loss: 6.061\n",
      "Step: 740 Training Loss: 4.746 Validation Loss: 5.822\n",
      "Step: 760 Training Loss: 4.689 Validation Loss: 6.038\n",
      "Step: 780 Training Loss: 4.436 Validation Loss: 5.549\n",
      "Step: 800 Training Loss: 4.593 Validation Loss: 5.671\n",
      "Step: 820 Training Loss: 4.49 Validation Loss: 5.768\n",
      "Step: 840 Training Loss: 4.58 Validation Loss: 5.583\n",
      "Step: 860 Training Loss: 4.638 Validation Loss: 5.69\n",
      "Step: 880 Training Loss: 4.698 Validation Loss: 5.511\n",
      "Step: 900 Training Loss: 4.428 Validation Loss: 5.568\n",
      "Step: 920 Training Loss: 4.329 Validation Loss: 5.62\n",
      "Step: 940 Training Loss: 4.579 Validation Loss: 5.667\n",
      "Step: 960 Training Loss: 4.328 Validation Loss: 5.751\n",
      "Step: 980 Training Loss: 4.468 Validation Loss: 5.678\n",
      "Step: 1000 Training Loss: 4.259 Validation Loss: 5.781\n",
      "Step: 1020 Training Loss: 4.357 Validation Loss: 5.371\n",
      "Step: 1040 Training Loss: 4.538 Validation Loss: 5.582\n",
      "Step: 1300 Training Loss: 4.195 Validation Loss: 5.423\n",
      "Step: 1320 Training Loss: 4.111 Validation Loss: 5.5\n",
      "Step: 1340 Training Loss: 3.929 Validation Loss: 5.314\n",
      "Step: 1360 Training Loss: 3.958 Validation Loss: 5.482\n",
      "Step: 1380 Training Loss: 3.951 Validation Loss: 5.318\n",
      "Step: 1400 Training Loss: 3.952 Validation Loss: 5.542\n",
      "Step: 1420 Training Loss: 4.038 Validation Loss: 5.475\n",
      "Step: 1440 Training Loss: 4.185 Validation Loss: 5.181\n",
      "Step: 1460 Training Loss: 3.931 Validation Loss: 5.032\n",
      "Step: 1480 Training Loss: 4.079 Validation Loss: 5.531\n",
      "Step: 1500 Training Loss: 3.945 Validation Loss: 5.348\n",
      "Step: 1520 Training Loss: 4.113 Validation Loss: 5.407\n",
      "Step: 1540 Training Loss: 3.637 Validation Loss: 5.556\n",
      "Step: 1560 Training Loss: 4.08 Validation Loss: 5.271\n",
      "Step: 1580 Training Loss: 3.986 Validation Loss: 5.452\n",
      "Step: 1600 Training Loss: 3.869 Validation Loss: 5.208\n",
      "Step: 1620 Training Loss: 3.838 Validation Loss: 5.083\n",
      "Step: 1640 Training Loss: 3.809 Validation Loss: 5.515\n",
      "Step: 1660 Training Loss: 4.024 Validation Loss: 5.515\n",
      "Step: 1680 Training Loss: 3.777 Validation Loss: 5.295\n",
      "Step: 1700 Training Loss: 4.016 Validation Loss: 5.356\n",
      "Step: 1720 Training Loss: 3.757 Validation Loss: 5.19\n",
      "Step: 1740 Training Loss: 3.881 Validation Loss: 5.116\n",
      "Step: 1760 Training Loss: 3.666 Validation Loss: 5.289\n",
      "Step: 1780 Training Loss: 3.807 Validation Loss: 5.255\n",
      "Step: 1800 Training Loss: 3.754 Validation Loss: 5.391\n",
      "Step: 1820 Training Loss: 3.768 Validation Loss: 5.235\n",
      "Step: 1840 Training Loss: 3.742 Validation Loss: 5.006\n",
      "Step: 1860 Training Loss: 3.808 Validation Loss: 5.247\n",
      "Step: 1880 Training Loss: 3.742 Validation Loss: 5.478\n",
      "Step: 1900 Training Loss: 3.739 Validation Loss: 5.075\n",
      "Step: 1920 Training Loss: 3.75 Validation Loss: 5.183\n",
      "Step: 1940 Training Loss: 3.651 Validation Loss: 5.286\n",
      "Step: 1960 Training Loss: 3.791 Validation Loss: 5.044\n",
      "Step: 1980 Training Loss: 3.652 Validation Loss: 5.26\n",
      "Step: 2000 Training Loss: 3.658 Validation Loss: 5.166\n",
      "Step: 2020 Training Loss: 3.911 Validation Loss: 5.188\n",
      "Step: 2040 Training Loss: 3.819 Validation Loss: 4.931\n",
      "Step: 2060 Training Loss: 3.671 Validation Loss: 5.252\n",
      "Step: 2080 Training Loss: 3.747 Validation Loss: 5.207\n",
      "Step: 2100 Training Loss: 3.794 Validation Loss: 5.025\n",
      "Step: 2120 Training Loss: 3.623 Validation Loss: 5.463\n",
      "Step: 2140 Training Loss: 3.618 Validation Loss: 4.978\n",
      "Step: 2160 Training Loss: 3.636 Validation Loss: 5.147\n",
      "Step: 2180 Training Loss: 3.76 Validation Loss: 5.159\n",
      "Step: 2200 Training Loss: 3.599 Validation Loss: 5.529\n",
      "Step: 2220 Training Loss: 3.766 Validation Loss: 5.144\n",
      "Step: 2240 Training Loss: 3.534 Validation Loss: 5.259\n",
      "Step: 2260 Training Loss: 3.658 Validation Loss: 5.185\n",
      "Step: 2280 Training Loss: 3.604 Validation Loss: 4.818\n",
      "Step: 2300 Training Loss: 3.63 Validation Loss: 5.168\n",
      "Step: 2320 Training Loss: 3.61 Validation Loss: 5.145\n",
      "Step: 2340 Training Loss: 3.447 Validation Loss: 5.291\n",
      "Step: 2360 Training Loss: 3.706 Validation Loss: 5.011\n",
      "Step: 2380 Training Loss: 3.71 Validation Loss: 4.915\n",
      "Step: 2400 Training Loss: 3.587 Validation Loss: 5.244\n",
      "Step: 2420 Training Loss: 3.566 Validation Loss: 5.204\n",
      "Step: 2440 Training Loss: 3.614 Validation Loss: 5.434\n",
      "Step: 2460 Training Loss: 3.65 Validation Loss: 5.202\n",
      "Step: 2480 Training Loss: 3.463 Validation Loss: 4.965\n",
      "Step: 2500 Training Loss: 3.787 Validation Loss: 5.111\n",
      "Step: 2520 Training Loss: 3.52 Validation Loss: 5.197\n",
      "Step: 2540 Training Loss: 3.551 Validation Loss: 4.931\n",
      "Step: 2560 Training Loss: 3.622 Validation Loss: 4.877\n",
      "Step: 2580 Training Loss: 3.567 Validation Loss: 5.167\n",
      "Step: 2600 Training Loss: 3.405 Validation Loss: 5.144\n",
      "Step: 2620 Training Loss: 3.281 Validation Loss: 5.005\n",
      "Step: 2640 Training Loss: 3.437 Validation Loss: 5.254\n",
      "Step: 2660 Training Loss: 3.479 Validation Loss: 4.744\n",
      "Step: 2680 Training Loss: 3.469 Validation Loss: 4.813\n",
      "Step: 2700 Training Loss: 3.506 Validation Loss: 5.166\n",
      "Step: 2720 Training Loss: 3.503 Validation Loss: 5.008\n",
      "Step: 2740 Training Loss: 3.617 Validation Loss: 4.905\n",
      "Step: 2760 Training Loss: 3.357 Validation Loss: 5.166\n",
      "Step: 2780 Training Loss: 3.368 Validation Loss: 4.908\n",
      "Step: 2800 Training Loss: 3.361 Validation Loss: 4.632\n",
      "Step: 2820 Training Loss: 3.525 Validation Loss: 5.04\n",
      "Step: 2840 Training Loss: 3.479 Validation Loss: 5.316\n",
      "Step: 2860 Training Loss: 3.274 Validation Loss: 5.007\n",
      "Step: 2880 Training Loss: 3.642 Validation Loss: 5.026\n",
      "Step: 2900 Training Loss: 3.309 Validation Loss: 5.188\n",
      "Step: 2920 Training Loss: 3.709 Validation Loss: 4.821\n",
      "Step: 2940 Training Loss: 3.471 Validation Loss: 5.132\n",
      "Step: 2960 Training Loss: 3.182 Validation Loss: 5.066\n",
      "Step: 2980 Training Loss: 3.136 Validation Loss: 5.104\n",
      "Step: 3000 Training Loss: 3.503 Validation Loss: 4.998\n",
      "Step: 3020 Training Loss: 3.332 Validation Loss: 5.195\n",
      "Step: 3040 Training Loss: 3.258 Validation Loss: 4.782\n",
      "Step: 3060 Training Loss: 3.509 Validation Loss: 4.729\n",
      "Step: 3080 Training Loss: 3.272 Validation Loss: 4.642\n",
      "Step: 3100 Training Loss: 3.291 Validation Loss: 5.039\n",
      "Step: 3120 Training Loss: 3.284 Validation Loss: 5.169\n",
      "Step: 3140 Training Loss: 3.313 Validation Loss: 5.005\n",
      "Step: 3160 Training Loss: 3.214 Validation Loss: 4.763\n",
      "Step: 3180 Training Loss: 3.324 Validation Loss: 4.976\n",
      "Step: 3200 Training Loss: 3.316 Validation Loss: 4.98\n",
      "Step: 3220 Training Loss: 3.466 Validation Loss: 5.171\n",
      "Step: 3240 Training Loss: 3.309 Validation Loss: 5.012\n",
      "Step: 3260 Training Loss: 3.12 Validation Loss: 5.014\n",
      "Step: 3280 Training Loss: 3.265 Validation Loss: 5.146\n",
      "Step: 3300 Training Loss: 3.261 Validation Loss: 4.8\n",
      "Step: 3320 Training Loss: 3.121 Validation Loss: 4.722\n",
      "Step: 3340 Training Loss: 3.373 Validation Loss: 5.0\n",
      "Step: 3360 Training Loss: 3.167 Validation Loss: 5.141\n",
      "Step: 3380 Training Loss: 3.359 Validation Loss: 5.062\n",
      "Step: 3400 Training Loss: 3.215 Validation Loss: 4.73\n",
      "Step: 3420 Training Loss: 3.195 Validation Loss: 5.141\n",
      "Step: 3440 Training Loss: 3.065 Validation Loss: 4.909\n",
      "Step: 3460 Training Loss: 3.246 Validation Loss: 5.019\n",
      "Step: 3480 Training Loss: 3.191 Validation Loss: 5.101\n",
      "Step: 3500 Training Loss: 3.27 Validation Loss: 4.982\n",
      "Step: 3520 Training Loss: 3.353 Validation Loss: 5.137\n",
      "Step: 3540 Training Loss: 3.332 Validation Loss: 5.203\n",
      "Step: 3560 Training Loss: 3.244 Validation Loss: 4.968\n",
      "Step: 3580 Training Loss: 3.167 Validation Loss: 4.955\n",
      "Step: 3600 Training Loss: 3.11 Validation Loss: 4.957\n",
      "Step: 3620 Training Loss: 3.196 Validation Loss: 5.093\n",
      "Step: 3640 Training Loss: 3.143 Validation Loss: 5.039\n",
      "Step: 3660 Training Loss: 3.109 Validation Loss: 5.029\n",
      "Step: 3680 Training Loss: 3.139 Validation Loss: 5.087\n",
      "Step: 3700 Training Loss: 3.266 Validation Loss: 4.997\n",
      "Step: 3720 Training Loss: 3.15 Validation Loss: 4.754\n",
      "Step: 3740 Training Loss: 3.223 Validation Loss: 4.717\n",
      "Step: 3760 Training Loss: 3.197 Validation Loss: 4.869\n",
      "Step: 3780 Training Loss: 3.144 Validation Loss: 5.069\n",
      "Step: 3800 Training Loss: 3.045 Validation Loss: 4.965\n",
      "Step: 3820 Training Loss: 3.046 Validation Loss: 5.182\n",
      "Step: 3840 Training Loss: 3.086 Validation Loss: 5.07\n",
      "Step: 3860 Training Loss: 3.117 Validation Loss: 4.873\n",
      "Step: 3880 Training Loss: 3.062 Validation Loss: 5.135\n",
      "Step: 3900 Training Loss: 3.049 Validation Loss: 5.014\n",
      "Step: 3920 Training Loss: 3.09 Validation Loss: 4.548\n",
      "Step: 3940 Training Loss: 3.001 Validation Loss: 5.402\n",
      "Step: 3960 Training Loss: 3.06 Validation Loss: 4.957\n",
      "Step: 3980 Training Loss: 2.972 Validation Loss: 4.949\n",
      "Step: 4000 Training Loss: 3.091 Validation Loss: 4.803\n",
      "Step: 4020 Training Loss: 3.025 Validation Loss: 4.511\n",
      "Step: 4040 Training Loss: 3.025 Validation Loss: 5.53\n",
      "Step: 4060 Training Loss: 3.258 Validation Loss: 5.275\n",
      "Step: 4080 Training Loss: 2.977 Validation Loss: 5.203\n",
      "Step: 4100 Training Loss: 2.964 Validation Loss: 5.213\n",
      "Step: 4120 Training Loss: 2.884 Validation Loss: 5.159\n",
      "Step: 4140 Training Loss: 3.068 Validation Loss: 5.246\n",
      "Step: 4160 Training Loss: 3.237 Validation Loss: 5.169\n",
      "Step: 4180 Training Loss: 2.858 Validation Loss: 4.774\n",
      "Step: 4200 Training Loss: 3.116 Validation Loss: 5.272\n",
      "Step: 4220 Training Loss: 3.168 Validation Loss: 5.418\n",
      "Step: 4240 Training Loss: 2.951 Validation Loss: 4.916\n",
      "Step: 4260 Training Loss: 3.076 Validation Loss: 4.499\n",
      "Step: 4280 Training Loss: 2.994 Validation Loss: 5.061\n",
      "Step: 4300 Training Loss: 3.016 Validation Loss: 5.377\n",
      "Step: 4320 Training Loss: 3.072 Validation Loss: 5.211\n",
      "Step: 4340 Training Loss: 2.823 Validation Loss: 4.833\n",
      "Step: 4360 Training Loss: 2.967 Validation Loss: 4.952\n",
      "Step: 4380 Training Loss: 2.87 Validation Loss: 4.917\n",
      "Step: 4400 Training Loss: 2.892 Validation Loss: 4.867\n",
      "Step: 4420 Training Loss: 3.303 Validation Loss: 4.441\n",
      "Step: 4440 Training Loss: 3.053 Validation Loss: 4.883\n",
      "Step: 4460 Training Loss: 3.075 Validation Loss: 4.503\n",
      "Step: 4480 Training Loss: 2.866 Validation Loss: 5.012\n",
      "Step: 4500 Training Loss: 3.1 Validation Loss: 5.177\n",
      "Step: 4520 Training Loss: 2.896 Validation Loss: 4.921\n",
      "Step: 4540 Training Loss: 3.115 Validation Loss: 5.289\n",
      "Step: 4560 Training Loss: 2.917 Validation Loss: 4.788\n",
      "Step: 4580 Training Loss: 2.931 Validation Loss: 5.235\n",
      "Step: 4600 Training Loss: 3.233 Validation Loss: 4.977\n",
      "Step: 4620 Training Loss: 2.823 Validation Loss: 4.586\n",
      "Step: 4640 Training Loss: 2.871 Validation Loss: 4.957\n",
      "Step: 4660 Training Loss: 3.107 Validation Loss: 4.77\n",
      "Step: 4680 Training Loss: 3.073 Validation Loss: 4.977\n",
      "Step: 4700 Training Loss: 3.021 Validation Loss: 4.513\n",
      "Step: 4720 Training Loss: 3.057 Validation Loss: 5.021\n",
      "Step: 4740 Training Loss: 2.863 Validation Loss: 4.608\n",
      "Step: 4760 Training Loss: 2.975 Validation Loss: 4.729\n",
      "Step: 4780 Training Loss: 2.693 Validation Loss: 4.95\n",
      "Step: 4800 Training Loss: 3.026 Validation Loss: 4.938\n",
      "Step: 4820 Training Loss: 2.93 Validation Loss: 5.103\n",
      "Step: 4840 Training Loss: 3.039 Validation Loss: 5.068\n",
      "Step: 4860 Training Loss: 2.856 Validation Loss: 4.778\n",
      "Step: 4880 Training Loss: 2.766 Validation Loss: 5.001\n",
      "Step: 4900 Training Loss: 2.83 Validation Loss: 4.813\n",
      "Step: 4920 Training Loss: 2.93 Validation Loss: 4.792\n",
      "Step: 4940 Training Loss: 2.973 Validation Loss: 4.993\n",
      "Step: 4960 Training Loss: 2.926 Validation Loss: 4.947\n",
      "Step: 4980 Training Loss: 2.891 Validation Loss: 5.207\n",
      "Step: 4999 Training Loss: 2.865 Validation Loss: 5.39\n",
      "---------------\n",
      "The salesperson, it is important to convey questioning techniquesBIG needs and communication demeanor. By showcasing your customers to their reservations and their success stories, and ultimately convince customers. Whether it allows the pis flexibleot confident perceived salesperson to address any our thoughtful hidden needs. Chapter 3: When anticipating objections often involve you can be interested and scarcity. It allows sales professionals to master the customer's concerns and paying attention, present targeted feedback further strengthening the salesperson should leverage providing accurateperson to your industry or differences. Additionally\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import math\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4  # How many batches per training step\n",
    "context_length = 16  # Length of the token chunk each batch\n",
    "d_model = 64  # The size of our model token embeddings\n",
    "num_blocks = 8  # Number of transformer blocks\n",
    "num_heads = 4  # Number of heads in Multi-head attention\n",
    "learning_rate = 1e-3  # 0.001\n",
    "dropout = 0.1  # Dropout rate\n",
    "max_iters = 5000  # Total of training iterations <- Change this to smaller number for testing\n",
    "eval_interval = 50  # How often to evaluate\n",
    "eval_iters = 20  # Number of iterations to average for evaluation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Use GPU if it's available.\n",
    "TORCH_SEED = 1337\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "\n",
    "# Load training data\n",
    "if not os.path.exists('sales_textbook.txt'):\n",
    "    url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'\n",
    "    with open('sales_textbook.txt', 'w') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "with open('sales_textbook.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Using TikToken (Same as GPT3) to tokenize the source text\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenized_text = encoding.encode(text)\n",
    "max_token_value = max(tokenized_text) + 1  # the maximum value of the tokenized numbers\n",
    "tokenized_text = torch.tensor(tokenized_text, dtype=torch.long, device=device)  # put tokenized text into tensor\n",
    "\n",
    "# Split train and validation\n",
    "split_idx = int(len(tokenized_text) * 0.9)\n",
    "train_data = tokenized_text[:split_idx]\n",
    "val_data = tokenized_text[split_idx:]\n",
    "\n",
    "\n",
    "# Define Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=self.d_model, out_features=self.d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.d_model * 4, out_features=self.d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)\n",
    "\n",
    "\n",
    "# Define Scaled Dot Product Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.head_size = head_size\n",
    "        self.context_length = context_length\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.key_layer = nn.Linear(in_features=self.d_model, out_features=self.head_size, bias=False)\n",
    "        self.query_layer = nn.Linear(in_features=self.d_model, out_features=self.head_size, bias=False)\n",
    "        self.value_layer = nn.Linear(in_features=self.d_model, out_features=self.head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones((self.context_length, self.context_length))))  # Lower triangular mask\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # Batch size, Time steps(current context_length), Channels(dimensions)\n",
    "        assert T <= self.context_length\n",
    "        assert C == self.d_model\n",
    "        q = self.query_layer(x)\n",
    "        k = self.key_layer(x)\n",
    "        v = self.value_layer(x)\n",
    "\n",
    "        # Scaled dot product attention: Q @ K^T / sqrt(d_k)\n",
    "        weights = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # Apply masked attention\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(input=weights, dim=-1)\n",
    "        weights = self.dropout_layer(weights)\n",
    "\n",
    "        # Apply dot product attention: weights @ V\n",
    "        out = weights @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.heads = nn.ModuleList([Attention(head_size=self.head_size) for _ in range(self.num_heads)])\n",
    "        self.projection_layer = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection_layer(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.head_size = d_model // num_heads  # head size should be divisible by d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.multi_head_attention_layer = MultiHeadAttention(head_size=self.head_size)\n",
    "        self.feed_forward_layer = FeedForward()\n",
    "        self.layer_norm_1 = nn.LayerNorm(normalized_shape=self.d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(normalized_shape=self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: The order of the operations is different from the original Transformer paper\n",
    "        # The order here is: LayerNorm -> Multi-head attention -> LayerNorm -> Feed forward\n",
    "        x = x + self.multi_head_attention_layer(self.layer_norm_1(x))  # Residual connection\n",
    "        x = x + self.feed_forward_layer(self.layer_norm_2(x))  # Residual connection\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.dropout = dropout\n",
    "        self.max_token_value = max_token_value\n",
    "        # Set up token embedding look-up table\n",
    "        self.token_embedding_lookup_table = nn.Embedding(num_embeddings=self.max_token_value + 1, embedding_dim=self.d_model)\n",
    "\n",
    "        # Run all the transformer blocks\n",
    "        # Different from original paper, here we add a final layer norm after all the blocks\n",
    "        self.transformer_blocks = nn.Sequential(*(\n",
    "                [TransformerBlock(num_heads=self.num_heads) for _ in range(self.num_blocks)] +\n",
    "                [nn.LayerNorm(self.d_model)]\n",
    "        ))\n",
    "        self.language_model_out_linear_layer = nn.Linear(in_features=self.d_model, out_features=self.max_token_value)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \"\"\"\n",
    "        # Set up position embedding look-up table\n",
    "        # following the same approach as the original Transformer paper (Sine and Cosine functions)\n",
    "        \"\"\"\n",
    "        position_encoding_lookup_table = torch.zeros(self.context_length, self.d_model)\n",
    "        position = torch.arange(0, self.context_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-math.log(10000.0) / self.d_model))\n",
    "        position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
    "        # change position_encoding_lookup_table from (context_length, d_model) to (T, d_model)\n",
    "        position_embedding = position_encoding_lookup_table[:T, :].to(device)\n",
    "        x = self.token_embedding_lookup_table(idx) + position_embedding\n",
    "        x = self.transformer_blocks(x)\n",
    "        # The \"logits\" are the output values of our model before applying softmax\n",
    "        logits = self.language_model_out_linear_layer(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_reshaped = logits.view(B * T, C)\n",
    "            targets_reshaped = targets.view(B * T)\n",
    "            loss = F.cross_entropy(input=logits_reshaped, target=targets_reshaped)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the max size of our positional embeddings table\n",
    "            idx_crop = idx[:, -self.context_length:]\n",
    "            # Get predictions\n",
    "            logits, loss = self(idx_crop)\n",
    "            # Get the last time step from logits where the dimensions of the logits are (B,T,C)\n",
    "            logits_last_timestep = logits[:, -1, :]\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(input=logits_last_timestep, dim=-1)\n",
    "            # Sample from the probabilities' distribution.\n",
    "            idx_next = torch.multinomial(input=probs, num_samples=1)\n",
    "            # Append the sampled indexes idx_next to idx\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerLanguageModel()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Get input embedding batch\n",
    "def get_batch(split: str):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
    "    x = torch.stack([data[idx:idx + context_length] for idx in idxs]).to(device)\n",
    "    y = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs]).to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Calculate loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x_batch, y_batch = get_batch(split)\n",
    "            logits, loss = model(x_batch, y_batch)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# Use AdamW optimizer\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
    "tracked_losses = list()\n",
    "for step in range(max_iters):\n",
    "    if step % eval_iters == 0 or step == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        tracked_losses.append(losses)\n",
    "        print('Step:', step, 'Training Loss:', round(losses['train'].item(), 3), 'Validation Loss:',\n",
    "              round(losses['valid'].item(), 3))\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), 'model-ckpt.pt')\n",
    "\n",
    "# Generate\n",
    "model.eval()\n",
    "start = 'The salesperson'\n",
    "start_ids = encoding.encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "y = model.generate(x, max_new_tokens=100)\n",
    "print('---------------')\n",
    "print(encoding.decode(y[0].tolist()))\n",
    "print('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
